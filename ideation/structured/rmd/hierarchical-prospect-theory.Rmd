---
title: 'Hierarchical Prospect Theory Modeling Tests'
author: 'Dave Braun'
date: January 16, 2023
output:
    md_document:
        variant: markdown_github
        toc: TRUE
        df_print: 'paged'
        standalone: TRUE
knit: (function(input, encoding, output) {knitr::knit(input=input, encoding = encoding, output='../md/hierarchical-prospect-theory.md')})
---

```{r include = FALSE, setup}
## super useful docs on knitr
## https://yihui.org/knitr/options/#chunk_options

knitr::opts_knit$set(base.dir = '/home/dave/Dropbox (Lehigh University)/post_doc/professional/projects/gaita/ideation/structured/md/')
#knitr::opts_knit(base.dir = paste0(getwd(), '/../md/'))
knitr::opts_chunk$set(
                        fig.path = 'figures/', 
                        fig.align = 'center',
                        message = FALSE,
                        echo = FALSE,
                        warning = FALSE
                      )

options(mc.cores = parallel::detectCores())
```

```{r include = FALSE}
library(tidyverse)
library(latex2exp)
library(BH)
library(rstan)
library(gridExtra)
```

# Hierarchical Prospect Theory Modeling 

For the polished html, see [here](https://davebraun.net/gaita/ideation/structured/html/polished/hierarchical-prospect-theory.html)

I'm going to start by implementing traditional prospect theory (only the value function), meaning not trying to put things in the language of an effort cost function.

## Simulate data

### Functional form and subject parameters

Overall subjective value computation for a prospect (omitting the probability weighting function for simplicity):

$$
V = \Sigma~p\cdot v(x)
$$

Value function

$$
v(x) = \begin{cases}
-(x)^{\alpha_i} & \text{if } x \geq 0 \\
\lambda_i(-x)^{\alpha_i} & \text{if } x < 0
\end{cases}
$$
Where parameters $\lambda_i$ and $\alpha_i$ are specific to subject $i$.

Plotting the value function with $\alpha=2.5$:

```{r plot-value-function}
value <- function(x, alpha = 2.5, lambda = 2){
   if (x >= 0) {
       return((x)^alpha)
   } 
    return(-lambda * (-x)^alpha)
}

d <- data.frame(x = seq(-1, 1, by = .001))
d$sv <- sapply(d$x, FUN = value)

d %>% 
    ggplot(aes(x = x, y = sv)) + 
    geom_hline(yintercept = 0) + 
    geom_vline(xintercept = 0) + 
    geom_line(size = 1.5) + 
    # annotate('text', x = -.8, y = 0.5, label = 'Less effort demand') + 
    # annotate('text', x = .8, y = 0.5, label = 'More effort demand') + 
    # annotate('text', x = -.7, y = -1.85, label = 'Increased response time window') + 
    # annotate('text', x = .7, y = -1.85, label = 'Reduced response time window') + 
    labs(
        x = 'Objective reference-dependent outcome',
        y = 'Subjective Value'
    ) + 
    ylim(-2,2) + 
    theme_bw() + 
    theme(axis.text = element_blank(),
          axis.ticks = element_blank(),
          panel.grid = element_blank())
```


Logistic choice rule


$$
\begin{align*}
p(safe)_t = \frac{1}{1 + e^{-\varphi_i[V(safe_t) - V(risky_t) - \gamma_i]}}\\
Choice_t \sim Bernoulli[p(safe)_t]
\end{align*}
$$
Where $\varphi_i$ is a decision sensitivity parameter for subject $i$ in range $[0, \infty)$ where values closer to $0$ imply choosing at random. And $\gamma$ is a baseline risk preference parameter for subject $i$ where positive values suggest a greater bias towards risk seeking and negative values towards risk aversion.


### Group-level parameters

Subject-level parameters are random effects drawn from probability distributions with group-level parameters.

$$
\begin{align*}
\alpha_i \sim \mathcal{N}(\mu_{\alpha}, \sigma_{\alpha, i})\\
\lambda_i \sim \mathcal{N}(\mu_{\lambda}, \sigma_{\lambda, i})\\
\varphi_i \sim \mathcal{N}(\mu_{\varphi}, \sigma_{\varphi, i})\\
\gamma_i \sim \mathcal{N}(\mu_{\gamma}, \sigma_{\gamma, i})
\end{align*}
$$

So the full set of parameters to be estimated is $\{\mu_{\alpha}, \sigma_{\alpha,i}, \mu_{\lambda}, \sigma_{\lambda, i}, \mu_{\varphi}, \sigma_{\varphi, i}, \mu_{\gamma}, \sigma_{\gamma, i}\}$, where the $\mu_.$ parameters are the true population means and the $\sigma_.$ parameters are the subject-wise variances. 

*Aside:*

Down the road I can model some of these subject specific 

**Fixed values**

Adjust the $\alpha$ parameters to ensure that $1$ is outside the 95th percentiles.

```{r}
mu <- 3.5
sd <- 1
data <- data.frame(x = seq(0, 10, by = .001))
data$density <- dnorm(data$x, mu, sd)
data %>% 
    ggplot(aes(x = x, y = density)) + 
    geom_vline(xintercept = 1, linetype = 'dashed') + 
    geom_vline(xintercept = mu - sd*1.96, linetype = 'dashed', color = 'steelblue') + 
    geom_vline(xintercept = mu + sd*1.96, linetype = 'dashed', color = 'steelblue') + 
    annotate('text', x = 7.5, y = 0.3, label = paste0('mu = ', mu, ', sd = ', sd)) + 
    geom_density(stat = 'identity', fill = 'steelblue', alpha = 0.6) + 
    labs(
        x = latex2exp::TeX(r'($\alpha$)'),
        y = 'Density'
    ) +
    theme_bw()


alpha_m <- 3.5
alpha_sd <- 1
lambda_m <- 2
lambda_sd <- 1
phi_m <- .4
phi_sd <- .001
gamma_m <- 0
gamma_sd <- 0.5
```

$$
\begin{align*}
\alpha_{\mu} = `r alpha_m`\\
\alpha_{\sigma} = `r alpha_sd`\\
\lambda_{\mu} = `r lambda_m`\\
\lambda_{\sigma} = `r lambda_sd`\\
\varphi_{\mu} = `r phi_m`\\
\varphi_{\sigma} = `r phi_sd`\\
\gamma_{\mu} = `r gamma_m`\\
\gamma_{\sigma} = `r gamma_sd`
\end{align*}
$$


### Simulate from generative model

Let's assume the response time window is manipulated discretely, and standardized relative to individual subjects so it'll be on the scale of quantile. So let's define the space of possible prospects as ($(outcome_1, probability_1; \ldots;~outcome_n, probability_n)$):


$$
\begin{align*}
\text{Moderate gain: }[(0, 0.5; 0.5, 0.5), (0.25, 1)]\\
\text{Moderate loss: }[(0, 0.5; -0.5, 0.5), (-0.25, 1)]\\
\text{Extreme gain: }[(0.5, 0.5; 1, 0.5), (0.75, 1)]\\
\text{Extreme loss: }[(-0.5, 0.5; -1, 0.5), (-0.75, 1)]\\
\end{align*}
$$


```{r run-simulation}
d <- data.frame(risky1 = seq(-1, 1, by = 0.5), 
                   risky2 = c(-0.5, 0, 0, 0, 0.5),
                   safe = c(-0.75, -.25, 0, .25, 0.75))
d <- d[d$risky1 != 0,]

N <- 100
trials_per_subject <- 1000/4
d <- do.call('rbind', replicate(trials_per_subject*N, d, simplify = FALSE))
d$subject <- rep(1:N, each=trials_per_subject)


## parameters
parameters <- list(
    alpha_m = 3.5,
    alpha_sd = 1.5,
    lambda_m = 2,
    lambda_sd = 1,
    phi_m = 7,
    phi_sd = 4,
    gamma_m = 0,
    gamma_sd = .5)

## dummy p weight parameter
w <- 1

## multiplier
multiplier <- 1

## functions
p_choose_safe <- function(Vrisky, Vsafe, phi, gamma) {
    return(1 / (1 + exp(-phi * (Vsafe - Vrisky - gamma))))
}

V <- function(x, p, alpha, lambda) {
    out <- 0
    for (outcome in x) {
        out <- out + (pweight(p, w=w) * value(outcome, alpha = alpha, lambda = lambda)) 
    }
    return(out)
}

value <- function(x, alpha, lambda) {
    if (x >= 0) {
        return(x^alpha)
    } 
    return(-lambda * (-x)^alpha)
}

# cost <- function(x, alpha, lambda){
#    if (x >= 0) {
#        return(-(x)^alpha)
#    } 
#     return(lambda * (-x)^alpha)
# }


pweight <- function(p, w) {
    
    if (p == 1){
        return(1)
    }
    return(p * w)
}

choose <- function(risky1, risky2, safe, alpha, lambda, phi, gamma, mul = multiplier) {
    Vrisky <- V(x = c(risky1, risky2)*mul, p = 0.5, alpha, lambda)
    Vsafe <- V(x = safe*mul, p = 1, alpha, lambda)
    return(p_choose_safe(Vrisky, Vsafe, phi, gamma))
}

determine_parameters <- function(d, parameters) {
    N <- length(unique(d$subject))
    param_df <- data.frame(subject = 1:N)
    
    for (param in c('alpha', 'lambda', 'phi', 'gamma')) {
        mu <- paste0(param, '_m')
        sd <- paste0(param, '_sd')
        param_df[, param] <- rnorm(N, parameters[[mu]], parameters[[sd]])
        ## set floor at zero for all except gamma
        if (param != 'gamma') {
            param_df[param_df[,param] < 0, param] <- 0
        }
    }
    
    return(param_df)    
}

## get subject specfic parameters
pd <- determine_parameters(d, parameters)

d <- d %>% 
    inner_join(pd)

d$ssd_p <- mapply(choose, d$risky1, d$risky2, d$safe, d$alpha, d$lambda, d$phi, d$gamma)
d$ssd <- rbinom(nrow(d), 1, d$ssd_p)
d %>% 
    mutate(framing = ifelse(risky1 > 0, 'Easier', 'Harder')) %>% 
    group_by(subject, framing) %>% 
    summarize(ssd = mean(ssd)) %>% 
    ggplot(aes(x = framing, y = ssd)) + 
    geom_hline(yintercept = .5, linetype = 'dotted') + 
    stat_summary(fun = 'mean', geom='bar', fill = 'steelblue', alpha = .6, color = 'black') + 
    geom_errorbar(stat = 'summary', fun.y = 'sd', width = .2) + 
    geom_jitter(alpha = .4, height = 0, width = .05) + 
    geom_line(aes(group = subject), linetype = 'dashed', alpha = .6) + 
    ylim(0,1) + 
    labs(
        x = 'Framing', 
        y = 'Proportion Selection Safe Deck'
    ) + 
    theme_bw() + 
    theme(axis.ticks = element_blank(),
          panel.grid = element_blank())

d <- d %>% 
    select(-alpha:-gamma)
```
 




## Modeling

```{stan output.var = 'prospect_model', eval = FALSE}
// less informative priors and no probability weighting
data {
    int N;
    real risky1[N];
    real risky2[N];
    real safe[N];
    int choice[N];
    int ind[N]; // subject index
    int nsubj;
}

parameters {
    real<lower=0> alpha_mu;
    real<lower=0> alpha_sd;
    real<lower=0> lambda_mu;
    real<lower=0> lambda_sd;
    real<lower=0> phi_mu;
    real<lower=0> phi_sd;
    real gamma_mu;
    real<lower=0> gamma_sd;
    real alpha_sample[nsubj];
    real lambda_sample[nsubj];
    real phi_sample[nsubj];
    real gamma_sample[nsubj];
}

transformed parameters {
    real alpha_applied[nsubj];
    real lambda_applied[nsubj];
    real phi_applied[nsubj];
    real gamma_applied[nsubj];
    
    for (s in 1:nsubj) {
        alpha_applied[s] = exp(alpha_sample[s]);
        lambda_applied[s] = exp(lambda_sample[s]);
        phi_applied[s] = exp(phi_sample[s]);
        gamma_applied[s] = exp(gamma_sample[s]);
    }
}

model {
    real sv[N];
    real risky1v;
    real risky2v;
    real safev;
    
    // priors
    lambda_mu ~ uniform(0, 10);
    alpha_mu ~ uniform(0, 10);
    phi_mu ~ uniform(0, 10);
    gamma_mu ~ uniform(0, 10);
    lambda_sd ~ cauchy(0, 2.5);
    alpha_sd ~ cauchy(0, 2.5);
    phi_sd ~ cauchy(0, 2.5);
    gamma_sd ~ cauchy(0, 2.5);
    
    alpha_sample ~ normal(alpha_mu, alpha_sd);
    lambda_sample ~ normal(lambda_mu, lambda_sd);
    phi_sample ~ normal(phi_mu, phi_sd);
    gamma_sample ~ normal(gamma_mu, gamma_sd);
    
    
    for (i in 1:N) {
        if (risky1[i] < 0) {
            risky1v = -lambda_applied[ind[i]] * ((-risky1[i])^alpha_applied[ind[i]]) * 0.5 ;
            risky2v = -lambda_applied[ind[i]] * ((-risky2[i])^alpha_applied[ind[i]]) * 0.5;
            safev = -lambda_applied[ind[i]] * (-safe[i])^alpha_applied[ind[i]];
        } else {
            risky1v = (risky1[i]^alpha_applied[ind[i]]) * 0.5;
            risky2v = (risky2[i]^alpha_applied[ind[i]]) * 0.5;
            safev = (safe[i]^alpha_applied[ind[i]]);
        }
        sv[i] = 1 / (1 + exp(-phi_applied[ind[i]] * (safev - (risky1v + risky2v) - gamma_applied[ind[i]])));
    }
    choice ~ bernoulli(sv);
}
```



```{r, eval = FALSE}
fit <- sampling(prospect_model,
            data = list(N = nrow(d),
                        risky1 = d$risky1*multiplier,
                        risky2 = d$risky2*multiplier,
                        safe = d$safe*multiplier,
                        choice = d$ssd,
                        nsubj = length(unique(d$subject)),
                        ind = d$subject),
            warmup = 750,
            iter = 2000,
            chains = 3)
```

I'll have to look into how to make this more efficient.

```{r, eval = FALSE}
print(fit)
```
```{r, eval = FALSE}
result <- as.data.frame(fit)
result$iteration <- 1:(nrow(result))
result <- result %>% 
    select(-lp__) %>% 
    gather(parameter, estimate, lambda:phi)
```


```{r model-results, eval = FALSE}
params <- c(
    alpha = alpha,
    lambda = lambda,
    phi = phi
)
params <- data.frame(parameter = names(params), ground_truth = params)
params <- result %>% 
    group_by(parameter) %>% 
    summarize(mean = mean(estimate), lower = quantile(estimate, probs = .025),
              upper = quantile(estimate, probs = .975), sd = sd(estimate)) %>% 
    inner_join(params)

result %>% 
    ggplot(aes(x = iteration, y = estimate)) + 
    geom_line(alpha = .6) + 
    geom_hline(data = params, aes(yintercept = ground_truth), 
               linetype = 'dashed', color = 'steelblue', size = 2) +
    facet_wrap(~parameter, scales = 'free') + 
    theme_bw() + 
    theme(axis.ticks = element_blank(),
          panel.grid = element_blank(),
          strip.background = element_rect(color = 'black', fill = 'white'))

result %>% 
    ggplot(aes(x = estimate)) + 
    geom_histogram(fill = 'steelblue', alpha = .6) + 
    geom_vline(data = params, aes(xintercept = mean), linetype = 'dashed', color = 'yellow') + 
    geom_vline(data = params, aes(xintercept = lower), linetype = 'dashed', color = 'yellow') + 
    geom_vline(data = params, aes(xintercept = upper), linetype = 'dashed', color = 'yellow') + 
    geom_vline(data = params, aes(xintercept = ground_truth), linetype = 'dashed', color = 'green') +
    labs(
        x = 'Estimate',
        y = 'Count'
    ) + 
    facet_wrap(~parameter, scales = 'free') +
    theme_bw() + 
    theme(strip.background = element_rect(color = 'black', fill = 'white'))
```


Estimation of $\gamma$ needs to be constrained in the range $[0, 1]$ in order for the model to perform well. And I think there's sufficient motivation to not worry about trying to estimate $\gamma$ because of its correlations with other parameters, especially $\alpha$. And I can use [this](https://osf.io/npd54) paper as a precedent for omitting $\gamma$.

I need to look really carefully at how the likelihood function is being defined.
These posterior distributions are stable but mostly reliably missing the true
parameter values.


Realized my wonky cost function coding was making things backwards.

OH- the multiplier was making things all screwy because I was only applying it to the data generation and not to the model input. Ran a test to confirm that, if scaling is consistent, estimation of the parameters is robust to the scaling of inputs, which is nice and what you would expect / hope for.










